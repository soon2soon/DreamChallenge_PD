{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset is now located at: /home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 \n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_rotation_x_\",\n",
    "    \"body_rotation_y_\",\n",
    "    \"body_rotation_z_\",\n",
    "#     \"body_attitude_x_\",\n",
    "#     \"body_attitude_y_\",\n",
    "#     \"body_attitude_z_\",\n",
    "    \"body_gravity_x_\",\n",
    "    \"body_gravity_y_\",\n",
    "    \"body_gravity_z_\"\n",
    "]\n",
    "\n",
    "# INPUT_SIGNAL_TYPES = [\n",
    "#     \"body_outbound_acc_x_\",\n",
    "#     \"body_outbound_acc_y_\",\n",
    "#     \"body_outbound_acc_z_\",\n",
    "#     \"body_outbound_rotation_x_\",\n",
    "#     \"body_outbound_rotation_y_\",\n",
    "#     \"body_outbound_rotation_z_\",\n",
    "#     \"body_outbound_gravity_x_\",\n",
    "#     \"body_outbound_gravity_y_\",\n",
    "#     \"body_outbound_gravity_z_\"\n",
    "#     \"body_return_acc_x_\",\n",
    "#     \"body_return_acc_y_\",\n",
    "#     \"body_return_acc_z_\",\n",
    "#     \"body_return_rotation_x_\",\n",
    "#     \"body_return_rotation_y_\",\n",
    "#     \"body_return_rotation_z_\",\n",
    "#     \"body_return_gravity_x_\",\n",
    "#     \"body_return_gravity_y_\",\n",
    "#     \"body_return_gravity_z_\"\n",
    "# ]\n",
    "\n",
    "# Output classes\n",
    "LABELS = [\n",
    "    \"FALSE\",\n",
    "    \"TRUE\" \n",
    "] \n",
    "\n",
    "\n",
    "\n",
    "DATASET_PATH = \"/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/\"\n",
    "\n",
    "\n",
    "DATASET_FINAL_TEST_PATH = \"/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_test/\"\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_acc_x_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_acc_y_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_acc_z_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_rotation_x_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_rotation_y_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_rotation_z_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_gravity_x_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_gravity_y_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/body_gravity_z_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_acc_x_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_acc_y_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_acc_z_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_rotation_x_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_rotation_y_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_rotation_z_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_gravity_x_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_gravity_y_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/body_gravity_z_test.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/train/y_train.txt\n",
      "/home/soon2soon/Notebooks/DreamChallenge/data/processed_data_train/test/y_test.txt\n",
      "Dataset load done.\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        print(signal_type_path)\n",
    "        \n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "                \n",
    "            ]]\n",
    "        )\n",
    "        \n",
    "        file.close()\n",
    "\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_test_final_signals_paths = [\n",
    "    DATASET_FINAL_TEST_PATH + 'test/' + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "# X_test_final = load_X(X_test_final_signals_paths)\n",
    "\n",
    "\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    print(y_path)\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    return y_\n",
    "\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "# y_test_final_path = DATASET_FINAL_TEST_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "# y_test_final = load_y(y_test_final_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset load done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal Parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ training_data_count 189434\n",
      "+ test_data_count 3973\n",
      "+ n_steps 512\n",
      "+ n_input 9\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(3973, 512, 9) (3973, 1) 0.0266799 0.65206\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # training series (with 50% overlap between each serie)\n",
    "\n",
    "\n",
    "print(\"+ training_data_count\",training_data_count )\n",
    "\n",
    "test_data_count = len(X_test)  # number of testing series\n",
    "n_steps = len(X_train[0])  # timesteps per series\n",
    "n_input = len(X_train[0][0])  # number of input parameters per timestep\n",
    "\n",
    "print(\"+ test_data_count\",test_data_count )\n",
    "\n",
    "print(\"+ n_steps\", n_steps)\n",
    "print(\"+ n_input\", n_input)\n",
    "\n",
    "\n",
    "\n",
    "n_hidden = 96 # Hidden layer num of features\n",
    "n_classes = 2 \n",
    "n_hidden_3 = 32\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 20  # Loop xx times on the dataset\n",
    "batch_size = 300\n",
    "display_iter = 5000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.7) \n",
    "    \n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.7) \n",
    "\n",
    "    # Get lstm cell output\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "# def BiRNN_attn(x, weights, biases):\n",
    "\n",
    "#     # Prepare data shape to match `rnn` function requirements\n",
    "#     # Current data input shape: (batch_size, timesteps, n_input)\n",
    "#     # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "#     # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "#     x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "#     # Define lstm cells with tensorflow\n",
    "#     # Forward direction cell\n",
    "#     lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "#     drop = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, input_keep_prob = 0.7)\n",
    "    \n",
    "#     lstm_fw_cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length=4, state_is_tuple = True)\n",
    "\n",
    "    \n",
    "#     # Backward direction cell\n",
    "#     lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "#     drop = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell, input_keep_prob = 0.7)\n",
    "    \n",
    "#     lstm_bw_cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length=4, state_is_tuple = True)\n",
    "    \n",
    "    \n",
    "#     # Get lstm cell output\n",
    "    \n",
    "#     outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "\n",
    "\n",
    "#     # Linear activation, using rnn inner loop last output\n",
    "#     return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "def LSTM_RNN_3(_X, _weights, _biases):\n",
    "    # Reshape to split input to 3 channel List\n",
    "    _X = tf.transpose(_X, [2,0,1])\n",
    "    _X = tf.unstack(_X)\n",
    "    \n",
    "    _X_acc = tf.stack(_X[:3])\n",
    "    _X_rot = tf.stack(_X[3:6])\n",
    "    _X_gra = tf.stack(_X[6:])\n",
    "    # New shape: (3, batch_size, n_steps)\n",
    "    \n",
    "    _X_acc = tf.transpose(_X_acc, [2, 1, 0])\n",
    "    _X_rot = tf.transpose(_X_rot, [2, 1, 0])\n",
    "    _X_gra = tf.transpose(_X_gra, [2, 1, 0])\n",
    "\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X_acc = tf.reshape(_X_acc, [-1, 3])\n",
    "    _X_rot = tf.reshape(_X_rot, [-1, 3])\n",
    "    _X_gra = tf.reshape(_X_gra, [-1, 3])\n",
    "    # new shape: (3*batch_size, n_input)\n",
    "\n",
    "    # Linear activation\n",
    "    _X_acc = tf.nn.relu(tf.matmul(_X_acc, _weights['hidden_acc']) + _biases['hidden_acc'])\n",
    "    _X_rot = tf.nn.relu(tf.matmul(_X_rot, _weights['hidden_rot']) + _biases['hidden_rot'])\n",
    "    _X_gra = tf.nn.relu(tf.matmul(_X_gra, _weights['hidden_gra']) + _biases['hidden_gra'])\n",
    "    \n",
    "    _X = tf.concat([_X_acc, _X_rot, _X_gra],1)\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    lstm_cell_1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_1, input_keep_prob=0.7, output_keep_prob=0.7)\n",
    "    \n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    lstm_cell_2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_2, input_keep_prob=0.7, output_keep_prob=0.7) \n",
    "    \n",
    "        \n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out'], lstm_last_output\n",
    "\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    \n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) \n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    lstm_cell_1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_1, input_keep_prob=0.7, output_keep_prob=0.7)\n",
    "    \n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    lstm_cell_2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_2, input_keep_prob=0.7, output_keep_prob=0.7) \n",
    "    \n",
    "        \n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out'], lstm_last_output\n",
    " \n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "      \n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    \n",
    "    if n_values == 1:\n",
    "        n_values = 2\n",
    "    \n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "weights_3 = {\n",
    "    'hidden_acc': tf.Variable(tf.random_normal([3, n_hidden_3])), # Hidden layer weights\n",
    "    'hidden_rot': tf.Variable(tf.random_normal([3, n_hidden_3])),\n",
    "    'hidden_gra': tf.Variable(tf.random_normal([3, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases_3 = {\n",
    "    'hidden_acc': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'hidden_rot': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'hidden_gra': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# pred = LSTM_RNN_3(x, weights, biases)\n",
    "# pred, last_state = LSTM_RNN_3(x, weights_3, biases_3)\n",
    "pred, last_state = LSTM_RNN(x, weights, biases)\n",
    "# pred = BiRNN(x, weights_bi, biases)\n",
    "# pred = BiRNN_attn(x, weights_bi, biases) \n",
    "\n",
    "\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost) # RMS Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #300:   Batch Loss = 2.760908, Accuracy = 0.7033332586288452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.9690566062927246, Accuracy = 0.5502139925956726\n",
      "Training iter #15000:   Batch Loss = 2.505090, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.533428430557251, Accuracy = 0.5572615265846252\n",
      "Training iter #30000:   Batch Loss = 2.428706, Accuracy = 0.5066666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.3904006481170654, Accuracy = 0.5512207746505737\n",
      "Training iter #45000:   Batch Loss = 2.219393, Accuracy = 0.6633332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.2493796348571777, Accuracy = 0.6204379796981812\n",
      "Training iter #60000:   Batch Loss = 2.274346, Accuracy = 0.5966666340827942\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.1990041732788086, Accuracy = 0.624213457107544\n",
      "Training iter #75000:   Batch Loss = 2.004452, Accuracy = 0.6600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 2.027860164642334, Accuracy = 0.6219481825828552\n",
      "Training iter #90000:   Batch Loss = 1.941445, Accuracy = 0.6233332753181458\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.9491535425186157, Accuracy = 0.6234583854675293\n",
      "Training iter #105000:   Batch Loss = 1.938391, Accuracy = 0.6266666650772095\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.9037363529205322, Accuracy = 0.6221998929977417\n",
      "Training iter #120000:   Batch Loss = 1.813269, Accuracy = 0.6466666460037231\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7822887897491455, Accuracy = 0.6234583854675293\n",
      "Training iter #135000:   Batch Loss = 1.636923, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.7067489624023438, Accuracy = 0.6206897497177124\n",
      "Training iter #150000:   Batch Loss = 1.555376, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.641484022140503, Accuracy = 0.6206896305084229\n",
      "Training iter #165000:   Batch Loss = 1.589023, Accuracy = 0.6233333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5768934488296509, Accuracy = 0.6211930513381958\n",
      "Training iter #180000:   Batch Loss = 1.483529, Accuracy = 0.690000057220459\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.5246775150299072, Accuracy = 0.6237100958824158\n",
      "Training iter #195000:   Batch Loss = 1.426428, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.490830898284912, Accuracy = 0.6227033138275146\n",
      "Training iter #210000:   Batch Loss = 1.441638, Accuracy = 0.6066666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.424239993095398, Accuracy = 0.6247168779373169\n",
      "Training iter #225000:   Batch Loss = 1.326370, Accuracy = 0.7033333778381348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.4067487716674805, Accuracy = 0.6224515438079834\n",
      "Training iter #240000:   Batch Loss = 1.280188, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.3383853435516357, Accuracy = 0.6211930513381958\n",
      "Training iter #255000:   Batch Loss = 1.220354, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.2917497158050537, Accuracy = 0.6229549646377563\n",
      "Training iter #270000:   Batch Loss = 1.254294, Accuracy = 0.65666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.277836561203003, Accuracy = 0.6229549646377563\n",
      "Training iter #285000:   Batch Loss = 1.197209, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.217067837715149, Accuracy = 0.6229550242424011\n",
      "Training iter #300000:   Batch Loss = 1.053723, Accuracy = 0.7999999523162842\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1933737993240356, Accuracy = 0.6227032542228699\n",
      "Training iter #315000:   Batch Loss = 1.119771, Accuracy = 0.7066666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1605429649353027, Accuracy = 0.6221998929977417\n",
      "Training iter #330000:   Batch Loss = 1.098635, Accuracy = 0.653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1422960758209229, Accuracy = 0.6221998333930969\n",
      "Training iter #345000:   Batch Loss = 1.052636, Accuracy = 0.6633332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.1000462770462036, Accuracy = 0.6229549646377563\n",
      "Training iter #360000:   Batch Loss = 1.083464, Accuracy = 0.6100000143051147\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0702296495437622, Accuracy = 0.6224516034126282\n",
      "Training iter #375000:   Batch Loss = 1.000100, Accuracy = 0.7266666293144226\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.080087661743164, Accuracy = 0.6227033138275146\n",
      "Training iter #390000:   Batch Loss = 0.986937, Accuracy = 0.7133333086967468\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0339893102645874, Accuracy = 0.6201862096786499\n",
      "Training iter #405000:   Batch Loss = 0.941534, Accuracy = 0.7299999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 1.0003153085708618, Accuracy = 0.6257237195968628\n",
      "Training iter #420000:   Batch Loss = 0.889915, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9841524362564087, Accuracy = 0.6239618062973022\n",
      "Training iter #435000:   Batch Loss = 0.901683, Accuracy = 0.6933332681655884\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.963657021522522, Accuracy = 0.621444821357727\n",
      "Training iter #450000:   Batch Loss = 0.914080, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9428660869598389, Accuracy = 0.6219481825828552\n",
      "Training iter #465000:   Batch Loss = 0.848923, Accuracy = 0.7733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.923818826675415, Accuracy = 0.6254719495773315\n",
      "Training iter #480000:   Batch Loss = 0.906246, Accuracy = 0.6166666150093079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.9042655825614929, Accuracy = 0.6297509074211121\n",
      "Training iter #495000:   Batch Loss = 0.800264, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8957350254058838, Accuracy = 0.6221998929977417\n",
      "Training iter #510000:   Batch Loss = 0.888929, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8851549625396729, Accuracy = 0.6224516034126282\n",
      "Training iter #525000:   Batch Loss = 0.816173, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8597553968429565, Accuracy = 0.6189278364181519\n",
      "Training iter #540000:   Batch Loss = 0.830970, Accuracy = 0.6600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8592753410339355, Accuracy = 0.6234583854675293\n",
      "Training iter #555000:   Batch Loss = 0.741364, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8365065455436707, Accuracy = 0.6249685883522034\n",
      "Training iter #570000:   Batch Loss = 0.869341, Accuracy = 0.5433333516120911\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8288401365280151, Accuracy = 0.6221998929977417\n",
      "Training iter #585000:   Batch Loss = 0.753484, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8163450956344604, Accuracy = 0.6277372241020203\n",
      "Training iter #600000:   Batch Loss = 0.844825, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8075234293937683, Accuracy = 0.6191794872283936\n",
      "Training iter #615000:   Batch Loss = 0.720160, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8028252124786377, Accuracy = 0.6300025582313538\n",
      "Training iter #630000:   Batch Loss = 0.906771, Accuracy = 0.5933333039283752\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.8085061311721802, Accuracy = 0.6209413409233093\n",
      "Training iter #645000:   Batch Loss = 0.744073, Accuracy = 0.6766666173934937\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7791334390640259, Accuracy = 0.6272338032722473\n",
      "Training iter #660000:   Batch Loss = 0.736020, Accuracy = 0.7033333778381348\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7800617218017578, Accuracy = 0.625723659992218\n",
      "Training iter #675000:   Batch Loss = 0.575365, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7790573835372925, Accuracy = 0.6221998929977417\n",
      "Training iter #690000:   Batch Loss = 0.690188, Accuracy = 0.7166666984558105\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7611227035522461, Accuracy = 0.6201863288879395\n",
      "Training iter #705000:   Batch Loss = 0.605669, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7605241537094116, Accuracy = 0.6239617466926575\n",
      "Training iter #720000:   Batch Loss = 0.635550, Accuracy = 0.7799999117851257\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7439655065536499, Accuracy = 0.619934618473053\n",
      "Training iter #735000:   Batch Loss = 0.632464, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7371855974197388, Accuracy = 0.6224516034126282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #750000:   Batch Loss = 0.716459, Accuracy = 0.653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7258167266845703, Accuracy = 0.6274855136871338\n",
      "Training iter #765000:   Batch Loss = 0.682209, Accuracy = 0.7066666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7328992486000061, Accuracy = 0.6219481825828552\n",
      "Training iter #780000:   Batch Loss = 0.707365, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7204412817955017, Accuracy = 0.6209414005279541\n",
      "Training iter #795000:   Batch Loss = 0.588446, Accuracy = 0.7766667008399963\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7317416667938232, Accuracy = 0.6247168183326721\n",
      "Training iter #810000:   Batch Loss = 0.703408, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7187536358833313, Accuracy = 0.6247168779373169\n",
      "Training iter #825000:   Batch Loss = 0.649497, Accuracy = 0.7266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7275341153144836, Accuracy = 0.6214447617530823\n",
      "Training iter #840000:   Batch Loss = 0.676466, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7135533094406128, Accuracy = 0.6232066750526428\n",
      "Training iter #855000:   Batch Loss = 0.594888, Accuracy = 0.7733333706855774\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7045865058898926, Accuracy = 0.6227033138275146\n",
      "Training iter #870000:   Batch Loss = 0.616636, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7128293514251709, Accuracy = 0.6227033138275146\n",
      "Training iter #885000:   Batch Loss = 0.638823, Accuracy = 0.7399999499320984\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7057825326919556, Accuracy = 0.6237100958824158\n",
      "Training iter #900000:   Batch Loss = 0.730952, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7013046145439148, Accuracy = 0.6254719495773315\n",
      "Training iter #915000:   Batch Loss = 0.745946, Accuracy = 0.6433333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6918952465057373, Accuracy = 0.6254719495773315\n",
      "Training iter #930000:   Batch Loss = 0.740799, Accuracy = 0.5566666126251221\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6907832622528076, Accuracy = 0.6244651675224304\n",
      "Training iter #945000:   Batch Loss = 0.701172, Accuracy = 0.5966666340827942\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6903765797615051, Accuracy = 0.6249685287475586\n",
      "Training iter #960000:   Batch Loss = 0.733892, Accuracy = 0.6499999165534973\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7277270555496216, Accuracy = 0.6229549646377563\n",
      "Training iter #975000:   Batch Loss = 0.727737, Accuracy = 0.6066666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6887774467468262, Accuracy = 0.6284923553466797\n",
      "Training iter #990000:   Batch Loss = 0.606231, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6897257566452026, Accuracy = 0.6305059194564819\n",
      "Training iter #1005000:   Batch Loss = 0.583575, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6903584599494934, Accuracy = 0.6287440657615662\n",
      "Training iter #1020000:   Batch Loss = 0.678084, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6985836625099182, Accuracy = 0.6232067346572876\n",
      "Training iter #1035000:   Batch Loss = 0.598636, Accuracy = 0.7699999213218689\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6856027841567993, Accuracy = 0.6229549646377563\n",
      "Training iter #1050000:   Batch Loss = 0.670954, Accuracy = 0.6600000262260437\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6767747402191162, Accuracy = 0.6267304420471191\n",
      "Training iter #1065000:   Batch Loss = 0.577596, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6700496673583984, Accuracy = 0.6244652271270752\n",
      "Training iter #1080000:   Batch Loss = 0.647472, Accuracy = 0.6799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6752550005912781, Accuracy = 0.627737283706665\n",
      "Training iter #1095000:   Batch Loss = 0.645120, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6847398281097412, Accuracy = 0.6244651675224304\n",
      "Training iter #1110000:   Batch Loss = 0.616838, Accuracy = 0.6700000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6835934519767761, Accuracy = 0.6239618062973022\n",
      "Training iter #1125000:   Batch Loss = 0.539144, Accuracy = 0.7766666412353516\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6685296297073364, Accuracy = 0.6300025582313538\n",
      "Training iter #1140000:   Batch Loss = 0.685723, Accuracy = 0.6599999666213989\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6823795437812805, Accuracy = 0.622451663017273\n",
      "Training iter #1155000:   Batch Loss = 0.730434, Accuracy = 0.6233333349227905\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6901190280914307, Accuracy = 0.6292474269866943\n",
      "Training iter #1170000:   Batch Loss = 0.668810, Accuracy = 0.6266667246818542\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6904758810997009, Accuracy = 0.6224515438079834\n",
      "Training iter #1185000:   Batch Loss = 0.682436, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6679046154022217, Accuracy = 0.6294991374015808\n",
      "Training iter #1200000:   Batch Loss = 0.602418, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6671110391616821, Accuracy = 0.6302542090415955\n",
      "Training iter #1215000:   Batch Loss = 0.603394, Accuracy = 0.7800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6713699102401733, Accuracy = 0.632519543170929\n",
      "Training iter #1230000:   Batch Loss = 0.576497, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.671588122844696, Accuracy = 0.6297507882118225\n",
      "Training iter #1245000:   Batch Loss = 0.585068, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6666148900985718, Accuracy = 0.6274856328964233\n",
      "Training iter #1260000:   Batch Loss = 0.570294, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6677595973014832, Accuracy = 0.628240704536438\n",
      "Training iter #1275000:   Batch Loss = 0.623184, Accuracy = 0.7299999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6589193344116211, Accuracy = 0.6350365281105042\n",
      "Training iter #1290000:   Batch Loss = 0.662714, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6527926325798035, Accuracy = 0.6355399489402771\n",
      "Training iter #1305000:   Batch Loss = 0.585255, Accuracy = 0.753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6775123476982117, Accuracy = 0.6294991374015808\n",
      "Training iter #1320000:   Batch Loss = 0.528741, Accuracy = 0.7866666316986084\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6699344515800476, Accuracy = 0.6325196027755737\n",
      "Training iter #1335000:   Batch Loss = 0.622174, Accuracy = 0.736666738986969\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6612048149108887, Accuracy = 0.6287440061569214\n",
      "Training iter #1350000:   Batch Loss = 0.626959, Accuracy = 0.6533333659172058\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6746811866760254, Accuracy = 0.6221998929977417\n",
      "Training iter #1365000:   Batch Loss = 0.635888, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6766729354858398, Accuracy = 0.6227033138275146\n",
      "Training iter #1380000:   Batch Loss = 0.606934, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6669259071350098, Accuracy = 0.6229549646377563\n",
      "Training iter #1395000:   Batch Loss = 0.644694, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6739867329597473, Accuracy = 0.6279889345169067\n",
      "Training iter #1410000:   Batch Loss = 0.593235, Accuracy = 0.7266666889190674\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6799432039260864, Accuracy = 0.6227033138275146\n",
      "Training iter #1425000:   Batch Loss = 0.669888, Accuracy = 0.6433333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6697849631309509, Accuracy = 0.6254719495773315\n",
      "Training iter #1440000:   Batch Loss = 0.620292, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6647655963897705, Accuracy = 0.6219481229782104\n",
      "Training iter #1455000:   Batch Loss = 0.605866, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6690361499786377, Accuracy = 0.6249685883522034\n",
      "Training iter #1470000:   Batch Loss = 0.640705, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6525569558143616, Accuracy = 0.632519543170929\n",
      "Training iter #1485000:   Batch Loss = 0.546891, Accuracy = 0.7766666412353516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.654969334602356, Accuracy = 0.6267304420471191\n",
      "Training iter #1500000:   Batch Loss = 0.605393, Accuracy = 0.7233333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6523698568344116, Accuracy = 0.6425874829292297\n",
      "Training iter #1515000:   Batch Loss = 0.726304, Accuracy = 0.5633333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6743023991584778, Accuracy = 0.6201862692832947\n",
      "Training iter #1530000:   Batch Loss = 0.623409, Accuracy = 0.6633332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6571059823036194, Accuracy = 0.6244651675224304\n",
      "Training iter #1545000:   Batch Loss = 0.626497, Accuracy = 0.6566666960716248\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6534209847450256, Accuracy = 0.6536622643470764\n",
      "Training iter #1560000:   Batch Loss = 0.497790, Accuracy = 0.7866666316986084\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6617143750190735, Accuracy = 0.6385602951049805\n",
      "Training iter #1575000:   Batch Loss = 0.460384, Accuracy = 0.8233333230018616\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7066987752914429, Accuracy = 0.6292474865913391\n",
      "Training iter #1590000:   Batch Loss = 0.620127, Accuracy = 0.7233332991600037\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.653491735458374, Accuracy = 0.6453562378883362\n",
      "Training iter #1605000:   Batch Loss = 0.606381, Accuracy = 0.7299999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6557565927505493, Accuracy = 0.6224516034126282\n",
      "Training iter #1620000:   Batch Loss = 0.681183, Accuracy = 0.6166666150093079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6716798543930054, Accuracy = 0.6284923553466797\n",
      "Training iter #1635000:   Batch Loss = 0.538458, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6633756160736084, Accuracy = 0.6244652271270752\n",
      "Training iter #1650000:   Batch Loss = 0.672218, Accuracy = 0.6666666269302368\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6543170809745789, Accuracy = 0.6335263252258301\n",
      "Training iter #1665000:   Batch Loss = 0.684044, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.657833456993103, Accuracy = 0.6269821524620056\n",
      "Training iter #1680000:   Batch Loss = 0.588172, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6612573266029358, Accuracy = 0.624213457107544\n",
      "Training iter #1695000:   Batch Loss = 0.617630, Accuracy = 0.6666666269302368\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6473995447158813, Accuracy = 0.6327712535858154\n",
      "Training iter #1710000:   Batch Loss = 0.731409, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6770237684249878, Accuracy = 0.6227033138275146\n",
      "Training iter #1725000:   Batch Loss = 0.667059, Accuracy = 0.6866666674613953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6531841158866882, Accuracy = 0.6327712535858154\n",
      "Training iter #1740000:   Batch Loss = 0.590978, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6865533590316772, Accuracy = 0.6224515438079834\n",
      "Training iter #1755000:   Batch Loss = 0.538185, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6485767960548401, Accuracy = 0.6420841217041016\n",
      "Training iter #1770000:   Batch Loss = 0.658576, Accuracy = 0.65666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6453889012336731, Accuracy = 0.6516487002372742\n",
      "Training iter #1785000:   Batch Loss = 0.638065, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6600157618522644, Accuracy = 0.6249685883522034\n",
      "Training iter #1800000:   Batch Loss = 0.558566, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.64504075050354, Accuracy = 0.6312610507011414\n",
      "Training iter #1815000:   Batch Loss = 0.574645, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6475155353546143, Accuracy = 0.6438459753990173\n",
      "Training iter #1830000:   Batch Loss = 0.615049, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6637278199195862, Accuracy = 0.6300026178359985\n",
      "Training iter #1845000:   Batch Loss = 0.607222, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6653380393981934, Accuracy = 0.6383086442947388\n",
      "Training iter #1860000:   Batch Loss = 0.566172, Accuracy = 0.7299999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6359277367591858, Accuracy = 0.6589479446411133\n",
      "Training iter #1875000:   Batch Loss = 0.530791, Accuracy = 0.7866666913032532\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6454781293869019, Accuracy = 0.6488800048828125\n",
      "Training iter #1890000:   Batch Loss = 0.792264, Accuracy = 0.5533333420753479\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6948204040527344, Accuracy = 0.6252202987670898\n",
      "Training iter #1905000:   Batch Loss = 0.557196, Accuracy = 0.753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.660673201084137, Accuracy = 0.6440976858139038\n",
      "Training iter #1920000:   Batch Loss = 0.630187, Accuracy = 0.6633333563804626\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6506079435348511, Accuracy = 0.6466147303581238\n",
      "Training iter #1935000:   Batch Loss = 0.586713, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6548241376876831, Accuracy = 0.6519004106521606\n",
      "Training iter #1950000:   Batch Loss = 0.569970, Accuracy = 0.753333330154419\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6440800428390503, Accuracy = 0.6581928730010986\n",
      "Training iter #1965000:   Batch Loss = 0.610473, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6447163820266724, Accuracy = 0.6579411625862122\n",
      "Training iter #1980000:   Batch Loss = 0.584208, Accuracy = 0.7466666102409363\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6432278156280518, Accuracy = 0.6410772800445557\n",
      "Training iter #1995000:   Batch Loss = 0.724952, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6400917768478394, Accuracy = 0.6586962342262268\n",
      "Training iter #2010000:   Batch Loss = 0.618302, Accuracy = 0.6799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6394141912460327, Accuracy = 0.6589479446411133\n",
      "Training iter #2025000:   Batch Loss = 0.574786, Accuracy = 0.7299999594688416\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6884140968322754, Accuracy = 0.6332746148109436\n",
      "Training iter #2040000:   Batch Loss = 0.556723, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.631540834903717, Accuracy = 0.663226842880249\n",
      "Training iter #2055000:   Batch Loss = 0.594213, Accuracy = 0.7199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6506769061088562, Accuracy = 0.6433425545692444\n",
      "Training iter #2070000:   Batch Loss = 0.733717, Accuracy = 0.5666666030883789\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6530901193618774, Accuracy = 0.6456078886985779\n",
      "Training iter #2085000:   Batch Loss = 0.598197, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6604543924331665, Accuracy = 0.6244651675224304\n",
      "Training iter #2100000:   Batch Loss = 0.601111, Accuracy = 0.7066665887832642\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6489725112915039, Accuracy = 0.6446011066436768\n",
      "Training iter #2115000:   Batch Loss = 0.548951, Accuracy = 0.8100000023841858\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6497915983200073, Accuracy = 0.6380568742752075\n",
      "Training iter #2130000:   Batch Loss = 0.602543, Accuracy = 0.7199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6332660913467407, Accuracy = 0.6654921770095825\n",
      "Training iter #2145000:   Batch Loss = 0.552439, Accuracy = 0.7466666102409363\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6808618307113647, Accuracy = 0.632016122341156\n",
      "Training iter #2160000:   Batch Loss = 0.690053, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6437423229217529, Accuracy = 0.6534105539321899\n",
      "Training iter #2175000:   Batch Loss = 0.587664, Accuracy = 0.7099999785423279\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.639816164970398, Accuracy = 0.6534105539321899\n",
      "Training iter #2190000:   Batch Loss = 0.585584, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6549146175384521, Accuracy = 0.6534105539321899\n",
      "Training iter #2205000:   Batch Loss = 0.644806, Accuracy = 0.6699999570846558\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6594192385673523, Accuracy = 0.6375535726547241\n",
      "Training iter #2220000:   Batch Loss = 0.579559, Accuracy = 0.7233333587646484\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6622738838195801, Accuracy = 0.6385602951049805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2235000:   Batch Loss = 0.641309, Accuracy = 0.6399999856948853\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6327468156814575, Accuracy = 0.6722880601882935\n",
      "Training iter #2250000:   Batch Loss = 0.596862, Accuracy = 0.7199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6488665342330933, Accuracy = 0.648628294467926\n",
      "Training iter #2265000:   Batch Loss = 0.565466, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6292791962623596, Accuracy = 0.6670023202896118\n",
      "Training iter #2280000:   Batch Loss = 0.615657, Accuracy = 0.6799999475479126\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6495817303657532, Accuracy = 0.6305059194564819\n",
      "Training iter #2295000:   Batch Loss = 0.605275, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6454575061798096, Accuracy = 0.6607098579406738\n",
      "Training iter #2310000:   Batch Loss = 0.541988, Accuracy = 0.7966667413711548\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6483161449432373, Accuracy = 0.6513969898223877\n",
      "Training iter #2325000:   Batch Loss = 0.576596, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6348873376846313, Accuracy = 0.6727913618087769\n",
      "Training iter #2340000:   Batch Loss = 0.534538, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6465750336647034, Accuracy = 0.6664988994598389\n",
      "Training iter #2355000:   Batch Loss = 0.576255, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6382354497909546, Accuracy = 0.6677573919296265\n",
      "Training iter #2370000:   Batch Loss = 0.651299, Accuracy = 0.6633332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6316849589347839, Accuracy = 0.6720362901687622\n",
      "Training iter #2385000:   Batch Loss = 0.653884, Accuracy = 0.6766666769981384\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6674133539199829, Accuracy = 0.6456079483032227\n",
      "Training iter #2400000:   Batch Loss = 0.588311, Accuracy = 0.7300000190734863\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6486106514930725, Accuracy = 0.6559275984764099\n",
      "Training iter #2415000:   Batch Loss = 0.572672, Accuracy = 0.746666669845581\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6375894546508789, Accuracy = 0.6717846393585205\n",
      "Training iter #2430000:   Batch Loss = 0.685113, Accuracy = 0.6266666054725647\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6262174248695374, Accuracy = 0.6775736808776855\n",
      "Training iter #2445000:   Batch Loss = 0.670069, Accuracy = 0.5866667032241821\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6410841941833496, Accuracy = 0.6373017430305481\n",
      "Training iter #2460000:   Batch Loss = 0.623168, Accuracy = 0.6933333873748779\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6477981209754944, Accuracy = 0.6524037718772888\n",
      "Training iter #2475000:   Batch Loss = 0.503678, Accuracy = 0.8133333921432495\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6446446180343628, Accuracy = 0.6647369861602783\n",
      "Training iter #2490000:   Batch Loss = 0.581804, Accuracy = 0.6966666579246521\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.645306408405304, Accuracy = 0.6680091023445129\n",
      "Training iter #2505000:   Batch Loss = 0.697248, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6420477032661438, Accuracy = 0.6627234220504761\n",
      "Training iter #2520000:   Batch Loss = 0.543813, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6346057653427124, Accuracy = 0.6682608127593994\n",
      "Training iter #2535000:   Batch Loss = 0.537858, Accuracy = 0.76666659116745\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6422078609466553, Accuracy = 0.663730263710022\n",
      "Training iter #2550000:   Batch Loss = 0.580157, Accuracy = 0.7800000309944153\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6424857378005981, Accuracy = 0.6539139747619629\n",
      "Training iter #2565000:   Batch Loss = 0.600200, Accuracy = 0.6766666173934937\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.647761881351471, Accuracy = 0.6312609910964966\n",
      "Training iter #2580000:   Batch Loss = 0.573836, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6389087438583374, Accuracy = 0.6629751324653625\n",
      "Training iter #2595000:   Batch Loss = 0.615672, Accuracy = 0.70333331823349\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6452637314796448, Accuracy = 0.6513969898223877\n",
      "Training iter #2610000:   Batch Loss = 0.550518, Accuracy = 0.7599999308586121\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6510444283485413, Accuracy = 0.6612132787704468\n",
      "Training iter #2625000:   Batch Loss = 0.647892, Accuracy = 0.65666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6297265291213989, Accuracy = 0.6743016242980957\n",
      "Training iter #2640000:   Batch Loss = 0.627361, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.665981650352478, Accuracy = 0.6367984414100647\n",
      "Training iter #2655000:   Batch Loss = 0.644989, Accuracy = 0.6766666173934937\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6499329209327698, Accuracy = 0.6362950205802917\n",
      "Training iter #2670000:   Batch Loss = 0.668952, Accuracy = 0.6700000166893005\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6892571449279785, Accuracy = 0.6330229043960571\n",
      "Training iter #2685000:   Batch Loss = 0.547171, Accuracy = 0.7733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6742604374885559, Accuracy = 0.6282406449317932\n",
      "Training iter #2700000:   Batch Loss = 0.677286, Accuracy = 0.6433333158493042\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6329846978187561, Accuracy = 0.6727913022041321\n",
      "Training iter #2715000:   Batch Loss = 0.637403, Accuracy = 0.6633332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.641849935054779, Accuracy = 0.6453561782836914\n",
      "Training iter #2730000:   Batch Loss = 0.545929, Accuracy = 0.7533332705497742\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6331968307495117, Accuracy = 0.6670023202896118\n",
      "Training iter #2745000:   Batch Loss = 0.634503, Accuracy = 0.6899999380111694\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6226113438606262, Accuracy = 0.6793354749679565\n",
      "Training iter #2760000:   Batch Loss = 0.642369, Accuracy = 0.65666663646698\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6302441954612732, Accuracy = 0.6717845797538757\n",
      "Training iter #2775000:   Batch Loss = 0.692287, Accuracy = 0.653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6355212926864624, Accuracy = 0.6602064371109009\n",
      "Training iter #2790000:   Batch Loss = 0.533391, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.639406681060791, Accuracy = 0.6534105539321899\n",
      "Training iter #2805000:   Batch Loss = 0.639117, Accuracy = 0.6733333468437195\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6251282691955566, Accuracy = 0.6685125827789307\n",
      "Training iter #2820000:   Batch Loss = 0.565550, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6368998885154724, Accuracy = 0.6740498542785645\n",
      "Training iter #2835000:   Batch Loss = 0.516982, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6303750872612, Accuracy = 0.6790838241577148\n",
      "Training iter #2850000:   Batch Loss = 0.623191, Accuracy = 0.7099999189376831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6270123720169067, Accuracy = 0.6790838837623596\n",
      "Training iter #2865000:   Batch Loss = 0.587806, Accuracy = 0.7066666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6467451453208923, Accuracy = 0.6501384973526001\n",
      "Training iter #2880000:   Batch Loss = 0.519368, Accuracy = 0.8100000619888306\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6440465450286865, Accuracy = 0.6425875425338745\n",
      "Training iter #2895000:   Batch Loss = 0.602110, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6236942410469055, Accuracy = 0.6798389554023743\n",
      "Training iter #2910000:   Batch Loss = 0.551056, Accuracy = 0.7400000095367432\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6315048336982727, Accuracy = 0.6707777976989746\n",
      "Training iter #2925000:   Batch Loss = 0.552055, Accuracy = 0.7199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6235926747322083, Accuracy = 0.6790838241577148\n",
      "Training iter #2940000:   Batch Loss = 0.703714, Accuracy = 0.5999999642372131\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6360949873924255, Accuracy = 0.6647370457649231\n",
      "Training iter #2955000:   Batch Loss = 0.704451, Accuracy = 0.5899999737739563\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6315756440162659, Accuracy = 0.6692675948143005\n",
      "Training iter #2970000:   Batch Loss = 0.583519, Accuracy = 0.7133333086967468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6290846467018127, Accuracy = 0.6702743768692017\n",
      "Training iter #2985000:   Batch Loss = 0.575712, Accuracy = 0.7099999189376831\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6278688311576843, Accuracy = 0.6629751920700073\n",
      "Training iter #3000000:   Batch Loss = 0.560505, Accuracy = 0.7066666483879089\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6292041540145874, Accuracy = 0.676315188407898\n",
      "Training iter #3015000:   Batch Loss = 0.623837, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6383792161941528, Accuracy = 0.6566826701164246\n",
      "Training iter #3030000:   Batch Loss = 0.676963, Accuracy = 0.6166666150093079\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6666487455368042, Accuracy = 0.634029746055603\n",
      "Training iter #3045000:   Batch Loss = 0.603871, Accuracy = 0.6599999666213989\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6276256442070007, Accuracy = 0.6629750728607178\n",
      "Training iter #3060000:   Batch Loss = 0.631066, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6339485049247742, Accuracy = 0.6534105539321899\n",
      "Training iter #3075000:   Batch Loss = 0.712087, Accuracy = 0.6200000047683716\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6436713933944702, Accuracy = 0.659199595451355\n",
      "Training iter #3090000:   Batch Loss = 0.625518, Accuracy = 0.7433333992958069\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7111984491348267, Accuracy = 0.6516487002372742\n",
      "Training iter #3105000:   Batch Loss = 0.583356, Accuracy = 0.6833332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.619103729724884, Accuracy = 0.6883967518806458\n",
      "Training iter #3120000:   Batch Loss = 0.503958, Accuracy = 0.7633333802223206\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6496412754058838, Accuracy = 0.6561792492866516\n",
      "Training iter #3135000:   Batch Loss = 0.467912, Accuracy = 0.8499999642372131\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6245098114013672, Accuracy = 0.6760634183883667\n",
      "Training iter #3150000:   Batch Loss = 0.717237, Accuracy = 0.5866667032241821\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6307604908943176, Accuracy = 0.6639819145202637\n",
      "Training iter #3165000:   Batch Loss = 0.511028, Accuracy = 0.7799999713897705\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6927812695503235, Accuracy = 0.6272338628768921\n",
      "Training iter #3180000:   Batch Loss = 0.659898, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.66014564037323, Accuracy = 0.6481248736381531\n",
      "Training iter #3195000:   Batch Loss = 0.701157, Accuracy = 0.6199999451637268\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6460034847259521, Accuracy = 0.6461113095283508\n",
      "Training iter #3210000:   Batch Loss = 0.667408, Accuracy = 0.6333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.652289628982544, Accuracy = 0.644097626209259\n",
      "Training iter #3225000:   Batch Loss = 0.623143, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6582388877868652, Accuracy = 0.6380568742752075\n",
      "Training iter #3240000:   Batch Loss = 0.605921, Accuracy = 0.7199999690055847\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6507596969604492, Accuracy = 0.6508935689926147\n",
      "Training iter #3255000:   Batch Loss = 0.601097, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6650377511978149, Accuracy = 0.6315127611160278\n",
      "Training iter #3270000:   Batch Loss = 0.591401, Accuracy = 0.7366666793823242\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6343693137168884, Accuracy = 0.6677573919296265\n",
      "Training iter #3285000:   Batch Loss = 0.587460, Accuracy = 0.7233332991600037\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6445283889770508, Accuracy = 0.6438460350036621\n",
      "Training iter #3300000:   Batch Loss = 0.550192, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6458266973495483, Accuracy = 0.6602064371109009\n",
      "Training iter #3315000:   Batch Loss = 0.671508, Accuracy = 0.6333332657814026\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.631655216217041, Accuracy = 0.6690158843994141\n",
      "Training iter #3330000:   Batch Loss = 0.623574, Accuracy = 0.6999999284744263\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6973773241043091, Accuracy = 0.6227033138275146\n",
      "Training iter #3345000:   Batch Loss = 0.589479, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6736786961555481, Accuracy = 0.6227033138275146\n",
      "Training iter #3360000:   Batch Loss = 0.523808, Accuracy = 0.846666693687439\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6940373778343201, Accuracy = 0.6227033138275146\n",
      "Training iter #3375000:   Batch Loss = 0.695764, Accuracy = 0.6233332753181458\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6589796543121338, Accuracy = 0.6302542686462402\n",
      "Training iter #3390000:   Batch Loss = 0.805247, Accuracy = 0.5800000429153442\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6584758162498474, Accuracy = 0.6229549646377563\n",
      "Training iter #3405000:   Batch Loss = 0.737491, Accuracy = 0.64000004529953\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.7386621236801147, Accuracy = 0.6227033138275146\n",
      "Training iter #3420000:   Batch Loss = 0.630718, Accuracy = 0.6933333277702332\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6546985507011414, Accuracy = 0.624213457107544\n",
      "Training iter #3435000:   Batch Loss = 0.708347, Accuracy = 0.6066666841506958\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6630355715751648, Accuracy = 0.6249685883522034\n",
      "Training iter #3450000:   Batch Loss = 0.620387, Accuracy = 0.7033332586288452\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6540974378585815, Accuracy = 0.6284923553466797\n",
      "Training iter #3465000:   Batch Loss = 0.530797, Accuracy = 0.7566666603088379\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6593208909034729, Accuracy = 0.6350365281105042\n",
      "Training iter #3480000:   Batch Loss = 0.713372, Accuracy = 0.5633333325386047\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6319560408592224, Accuracy = 0.663730263710022\n",
      "Training iter #3495000:   Batch Loss = 0.688309, Accuracy = 0.5966666340827942\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6382393836975098, Accuracy = 0.6433425545692444\n",
      "Training iter #3510000:   Batch Loss = 0.528678, Accuracy = 0.7733333110809326\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6391265988349915, Accuracy = 0.6654921174049377\n",
      "Training iter #3525000:   Batch Loss = 0.622470, Accuracy = 0.6833332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.644835352897644, Accuracy = 0.6599547266960144\n",
      "Training iter #3540000:   Batch Loss = 0.566678, Accuracy = 0.7399998903274536\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6312115788459778, Accuracy = 0.6755600571632385\n",
      "Training iter #3555000:   Batch Loss = 0.673921, Accuracy = 0.6133333444595337\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6322689056396484, Accuracy = 0.6599547863006592\n",
      "Training iter #3570000:   Batch Loss = 0.570672, Accuracy = 0.7699999809265137\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6435108780860901, Accuracy = 0.6556758880615234\n",
      "Training iter #3585000:   Batch Loss = 0.546087, Accuracy = 0.7066665887832642\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6249761581420898, Accuracy = 0.6748049855232239\n",
      "Training iter #3600000:   Batch Loss = 0.557298, Accuracy = 0.7933332920074463\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6612901091575623, Accuracy = 0.6249685287475586\n",
      "Training iter #3615000:   Batch Loss = 0.618918, Accuracy = 0.7133333683013916\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6592473387718201, Accuracy = 0.6385602951049805\n",
      "Training iter #3630000:   Batch Loss = 0.619266, Accuracy = 0.7833333015441895\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6553723216056824, Accuracy = 0.6440976858139038\n",
      "Training iter #3645000:   Batch Loss = 0.624747, Accuracy = 0.6933333873748779\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6474389433860779, Accuracy = 0.675308346748352\n",
      "Training iter #3660000:   Batch Loss = 0.509213, Accuracy = 0.8033334016799927\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6412439942359924, Accuracy = 0.6722879409790039\n",
      "Training iter #3675000:   Batch Loss = 0.667965, Accuracy = 0.653333306312561\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6182370781898499, Accuracy = 0.6778253316879272\n",
      "Training iter #3690000:   Batch Loss = 0.641439, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.625822901725769, Accuracy = 0.6750566959381104\n",
      "Training iter #3705000:   Batch Loss = 0.566985, Accuracy = 0.7599999904632568\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.678855299949646, Accuracy = 0.6597030758857727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3720000:   Batch Loss = 0.612679, Accuracy = 0.6833332777023315\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6311423182487488, Accuracy = 0.6629751324653625\n",
      "Training iter #3735000:   Batch Loss = 0.646306, Accuracy = 0.6966665983200073\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6390164494514465, Accuracy = 0.6697710156440735\n",
      "Training iter #3750000:   Batch Loss = 0.639004, Accuracy = 0.6633332967758179\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6319790482521057, Accuracy = 0.6685125231742859\n",
      "Training iter #3765000:   Batch Loss = 0.556074, Accuracy = 0.7433333396911621\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6265339255332947, Accuracy = 0.6682607531547546\n",
      "Training iter #3780000:   Batch Loss = 0.719421, Accuracy = 0.6133332848548889\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 0.6196277141571045, Accuracy = 0.6750566363334656\n",
      "Optimization Finished!\n",
      "Model saved in file: /home/soon2soon/Notebooks/DreamChallenge/LSTM-Human-Activity-Recognition/tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "model_path = \"/home/soon2soon/Notebooks/DreamChallenge/LSTM-Human-Activity-Recognition/tmp/model.ckpt\"\n",
    "\n",
    "\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "save_path = saver.save(sess, model_path)\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# one_hot_predictions, accuracy, final_loss, l_h = sess.run(\n",
    "#     [pred, accuracy, cost, last_state],\n",
    "#     feed_dict={\n",
    "#         x: X_test,\n",
    "#         y: one_hot(y_test)\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"FINAL RESULT: \" + \\\n",
    "#       \"Batch Loss = {}\".format(final_loss) + \\\n",
    "#       \", Accuracy = {}\".format(accuracy))\n",
    "\n",
    "\n",
    "# np.savetxt(\"output_hidden_test.txt\", l_h)\n",
    "\n",
    "# print(\"hidden test write donw\")\n",
    "#####################\n",
    "\n",
    "\n",
    "# itter = 1\n",
    "# chunk_size = 500\n",
    "# while itter * chunk_size <= len(X_test_final):\n",
    "#     batch_xs =         extract_batch_size(X_test_final, step, chunk_size)\n",
    "#     batch_ys = one_hot(extract_batch_size(y_test_final, step, chunk_size))\n",
    "\n",
    "#     # Fit training using batch data\n",
    "\n",
    "#     a_t, _, l_h_t = sess.run(\n",
    "#         [accuracy, cost, last_state],\n",
    "#         feed_dict={\n",
    "#             x: batch_xs,\n",
    "#             y: batch_ys\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     print(\"FINAL RESULT: \" + \\\n",
    "#           \", Accuracy = {}\".format(a_t))\n",
    "\n",
    "#     np.savetxt('output_state_result/'+\"output_hidden_train\"+str(itter)+\".txt\", l_h_t)\n",
    "\n",
    "#     print(itter, \"hidden test final write donw!\")\n",
    "#     itter= itter+1\n",
    "    \n",
    "# print(\"iter done.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
